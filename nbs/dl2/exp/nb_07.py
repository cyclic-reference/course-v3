
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/07_NORMIES.ipynb

from exp.nb_06 import *
from torch import nn

class BabyBatchNormalizationLayer(nn.Module):
    def __init__(self, layerSize, momentum=0.1, epsilon=1e-5):
        super().__init__()
        self.momentum, self.epsilon, self.layerSize = momentum, epsilon, layerSize
        self.gamma = nn.Parameter(torch.ones(layerSize, 1, 1))
        self.beta = nn.Parameter(torch.zeros(layerSize, 1, 1))
        self.register_buffer("aggregatedSums", torch.zeros(1, layerSize, 1, 1))
        self.register_buffer("aggregatedSquares", torch.zeros(1, layerSize, 1, 1))
        self.register_buffer("batch", torch.tensor(0.))
        self.register_buffer("counts", torch.tensor(0.))
        self.register_buffer("step", torch.tensor(0.))
        self.register_buffer("bias", torch.tensor(0.))

    def forward(self, activations):
        normalizedActivations = self._normalizeActivations(activations)
        return self.gamma * normalizedActivations + self.beta

    def _normalizeActivations(self, activations):
        batchMean, batchVariance = self._getMeanAndVariance(activations)
        return (activations - batchMean) / (batchVariance + self.epsilon).sqrt()

    def _getMeanAndVariance(self, activations):
        self.aggregatedSums.detach_()
        self.aggregatedSquares.detach_()
        batchSize, numberOfChannels, *_ = activations.shape
        dimensions = (0, 2, 3)
        channelSum = activations.sum(dimensions, keepdim=True)
        channelSquared = (activations * activations).sum(dimensions, keepdim=True)

        counts = self.counts.new_tensor(activations.numel() / numberOfChannels)  # I have no Idea what this is a count of, it definently is NOT the number of elements in each channel, and it is not the count of all of the elements in the groups of channels.

        momentum = self.bias.new_tensor((1 - (1 - self.momentum) / math.sqrt(batchSize - 1)))

        self.aggregatedSums.lerp_(channelSum, momentum)
        self.aggregatedSquares.lerp_(channelSquared, momentum)
        self.counts.lerp_(counts, momentum)
        self.bias = self.bias * (1 - momentum) + momentum
        self.batch += batchSize
        self.step += 1

        return self._addBiasToMeanAnVariance()


    def _addBiasToMeanAnVariance(self):
        sums = self.aggregatedSums
        squares = self.aggregatedSquares
        counts = self.counts
        if self.step < 100:
            sums /= self.bias
            squares /= self.bias
            counts /= self.bias
        means = sums/counts
        variances = (squares/counts).sub_(means*means)
        if(bool(self.batch < 20)): variances.clamp_min_(0.01)
        return means, variances