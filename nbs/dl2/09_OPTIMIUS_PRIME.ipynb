{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "from dataclasses import dataclass\n",
    "from functools import reduce, partial\n",
    "import torch\n",
    "from torch import optim\n",
    "from nbs.dl2.exp.nb_02 import getMnistData, assertNearZero\n",
    "from nbs.dl2.exp.nb_03 import Dataset, createDataLoaders, accuracy\n",
    "from nbs.dl2.exp.nb_04 import DataBunch\n",
    "from nbs.dl2.exp.nb_05 import aggregateSchedulers, createCosineSchedulers, cosineScheduler\n",
    "from nbs.dl2.exp.nb_06 import normalizeVectors, createBetterConvolutionModel\n",
    "from nbs.dl2.exp.nb_07D import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "%matplotlib inline\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "xTraining, yTraining, xValidation, yValidation = getMnistData()\n",
    "xTrainingNormalized, xValidationNormalized = \\\n",
    "    normalizeVectors(xTraining, xValidation)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "assertNearZero(xTrainingNormalized.mean())\n",
    "assertNearZero(xValidationNormalized.mean())\n",
    "assertNearZero(1 - xTrainingNormalized.std())\n",
    "assertNearZero(1 - xValidationNormalized.std())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "layerSizes = [8, 16, 32, 64, 64]\n",
    "numberOfClasses = 10\n",
    "hiddenLayerSize = 75\n",
    "batchSize = 64"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "trainingDataSet, validationDataSet = Dataset(xTrainingNormalized[:10000], yTraining[:10000]), Dataset(xValidationNormalized[:10000], yValidation[:10000])\n",
    "trainingDataLoader, validationDataLoader = createDataLoaders(trainingDataSet, validationDataSet, batchSize)\n",
    "imageDataBunch = DataBunch(trainingDataLoader, validationDataLoader, numberOfClasses)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "class ProcessCancellationException(Exception): pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "def composeFunctions(funInput, functions): \n",
    "    return reduce(lambda accum, function: function(accum), \n",
    "                  functions, \n",
    "                  funInput)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "result = composeFunctions(0, [\n",
    "    lambda x: x + 1,\n",
    "    lambda x: x + 2,\n",
    "    lambda x: x + 3,\n",
    "])\n",
    "\n",
    "assert result == 6, \"Composition is wrong\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class HyperParameters:\n",
    "    learningRate: float = 1e-1\n",
    "    weightDecay: float = 0.\n",
    "    momentum: float = 0.9"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "#export\n",
    "def flatMap(function, items):\n",
    "    return reduce(lambda accum, manyItems: accum + manyItems,\n",
    "           list(map(function, items)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "result = flatMap(lambda n: list(map(lambda _: n+1, range(n+1))), \n",
    "        list(range(3)))\n",
    "\n",
    "assert result == [1, 2, 2, 3, 3, 3], \"Flat map did not work\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "class OptimizationFunction:\n",
    "    \n",
    "    def __call__(self, \n",
    "                 modelLayer, \n",
    "                 hyperParameters: HyperParameters):\n",
    "        return hyperParameters\n",
    "    \n",
    "class LearningRateAnnealer(OptimizationFunction):\n",
    "    def __init__(self, learningRateSupplier)-> None:\n",
    "        self._learningRateSupplier = learningRateSupplier\n",
    "        \n",
    "    def __call__(self, modelLayer, hyperParameters: HyperParameters):\n",
    "        hyperParameters.learningRate = self._learningRateSupplier()\n",
    "        return super().__call__(modelLayer, hyperParameters)\n",
    "    \n",
    "class LearningRateOptimizer(OptimizationFunction):\n",
    "    def __call__(self, \n",
    "                 modelLayer, \n",
    "                 hyperParameters: HyperParameters):\n",
    "        modelLayer.data.add_(-hyperParameters.learningRate, modelLayer.grad.data)\n",
    "        return super().__call__(modelLayer, hyperParameters)\n",
    "    \n",
    "class MomentumOptimizer(OptimizationFunction):\n",
    "    \n",
    "    def __init__(self)-> None:\n",
    "        super().__init__()\n",
    "        self._state = {}\n",
    "        \n",
    "    def __call__(self, \n",
    "                 modelLayer, \n",
    "                 hyperParameters: HyperParameters):\n",
    "        if modelLayer not in self._state:\n",
    "            self._state[modelLayer] = {\n",
    "                'averageGradient': torch.zeros_like(modelLayer.grad.data)\n",
    "            }\n",
    "        modelLayerState = self._state[modelLayer]\n",
    "        modelLayerState['averageGradient'].mul_(hyperParameters.momentum).add_(modelLayer.grad.data)\n",
    "        \n",
    "        modelLayer.data.add_(-hyperParameters.learningRate, modelLayerState['averageGradient'])\n",
    "        # modelLayer.data.add_(-hyperParameters.learningRate, modelLayer.grad.data)\n",
    "        return super().__call__(modelLayer, hyperParameters)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "** Weight Decay** is useful when you are having issues with over fitting the training data as the batches go on.\n",
    "Added weight decay makes the parameter weights smaller so they learn less, but the can generalize better."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "class WeightDecayOptimizer(OptimizationFunction):\n",
    "    def __call__(self, \n",
    "                 modelLayer, \n",
    "                 hyperParameters: HyperParameters):\n",
    "        modelLayer.data.mul_(1 - hyperParameters.learningRate * hyperParameters.weightDecay)\n",
    "        return super().__call__(modelLayer, hyperParameters)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, modelParameters, \n",
    "                 optimizationFunctions,\n",
    "                 hyperParameters: HyperParameters=HyperParameters(learningRate=0.5)):\n",
    "        super().__init__()\n",
    "        self._modelParameters = list(modelParameters)\n",
    "        self._hyperParameters = [hyperParameters for _ in self._modelParameters]\n",
    "        self._optimizationFunctions = optimizationFunctions\n",
    "        \n",
    "    def getLayersWithGradients(self):\n",
    "        return list(filter(lambda modelAndHyperParameters: modelAndHyperParameters[0].grad is not None, \n",
    "                           zip(self._modelParameters, self._hyperParameters)))\n",
    "    \n",
    "    def resetGradients(self):\n",
    "        for modelParameter, _ in self.getLayersWithGradients():\n",
    "            modelParameter.grad.detach_()\n",
    "            modelParameter.grad.zero_()\n",
    "    \n",
    "    def optimizeModel(self):\n",
    "        for modelParameter, hyperParameter in self.getLayersWithGradients():\n",
    "            composeFunctions(hyperParameter, \n",
    "                             map(lambda fun: partial(fun, modelParameter), \n",
    "                                 self._optimizationFunctions))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "convolutionalModelSR2 = createBetterConvolutionModel(numberOfClasses, layerSizes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 49
    }
   ],
   "source": [
    "optimus = Optimizer(convolutionalModelSR2.parameters(), [])\n",
    "len(optimus.getLayersWithGradients())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss = torch.nn.functional.cross_entropy(convolutionalModelSR2(xTrainingNormalized), yTraining)\n",
    "loss.backward()\n",
    "len(optimus.getLayersWithGradients())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class LearningRateRecorder:\n",
    "    def __init__(self, \n",
    "                 learningRateSupplier=lambda:0.5):\n",
    "        super().__init__()\n",
    "        self._aggregatedLearningRates = []\n",
    "        self._learningRateSupplier = learningRateSupplier\n",
    "    def __call__(self):\n",
    "        learningRateReturned = self._learningRateSupplier()\n",
    "        self._aggregatedLearningRates.append(learningRateReturned)\n",
    "        return learningRateReturned\n",
    "    \n",
    "    def plotLearningRates(self):\n",
    "        plotter.plot(self._aggregatedLearningRates)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Important Note**: PyTorch will NOT put the gradients on the parameters until backwards has been called on the model    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TrainingSubscriber(StatisticsSubscriber, \n",
    "                         HookedSubscriber):\n",
    "\n",
    "    def __init__(self,\n",
    "                 lossFunction=torch.nn.functional.cross_entropy,\n",
    "                 schedulingFunction=cosineScheduler(1e-1, 1e-6), \n",
    "                 optimizationFunctions=[]\n",
    "                 ):\n",
    "        super().__init__(name=\"Training\")\n",
    "        self._optimizer: Optimizer = None\n",
    "        self._optimizationFunctions = optimizationFunctions\n",
    "        self._schedulingFunction = schedulingFunction\n",
    "        self._lossFunction = lossFunction\n",
    "        \n",
    "        self._learningRateRecorder = LearningRateRecorder()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def plotLearningRate(self):\n",
    "        self._learningRateRecorder.plotLearningRates()\n",
    "        \n",
    "    def getOptimizationFunctions(self):\n",
    "        learningRateSupplier = lambda: self._schedulingFunction(self.calculateCurrentProgress())\n",
    "        self._learningRateRecorder = LearningRateRecorder(learningRateSupplier )\n",
    "        return [\n",
    "            WeightDecayOptimizer(),\n",
    "            LearningRateAnnealer(self._learningRateRecorder),\n",
    "            MomentumOptimizer(),\n",
    "            # LearningRateOptimizer()  \n",
    "        ]\n",
    "\n",
    "    def preModelTeach(self, model, epochs):\n",
    "        super().preModelTeach(model, epochs)\n",
    "        self._optimizer = Optimizer(model.parameters(), \n",
    "                                    self.getOptimizationFunctions(),\n",
    "                                    HyperParameters(\n",
    "                                        weightDecay=0.0,\n",
    "                                        learningRate=self._schedulingFunction(0)\n",
    "                                    ))\n",
    "        self._totalEpochs = epochs\n",
    "\n",
    "    def postBatchEvaluation(self, predictions, validationData):\n",
    "        super().postBatchEvaluation(predictions, validationData)\n",
    "        calculatedLoss = self._lossFunction(predictions, validationData)\n",
    "        self._teachModel(calculatedLoss)\n",
    "        self.postBatchLossConsumption(calculatedLoss)\n",
    "\n",
    "    def _teachModel(self, loss):\n",
    "        loss.backward() # adds auto gradients to model parameters\n",
    "        self._optimizer.optimizeModel()\n",
    "        self._optimizer.resetGradients() "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TeacherOptimized:\n",
    "    def __init__(self,\n",
    "                 dataBunch,\n",
    "                 trainingSubscriber: TrainingSubscriber,\n",
    "                 validationSubscriber: ValidationSubscriber):\n",
    "        self._dataBunch = dataBunch\n",
    "        self._trainingSubscriber = trainingSubscriber\n",
    "        self._validationSubscriber = validationSubscriber\n",
    "\n",
    "    def teachModel(self, model, numberOfEpochs):\n",
    "        self._notifiyPreTeach(model, numberOfEpochs)\n",
    "        for epoch in range(numberOfEpochs):\n",
    "            self._trainModel(model,\n",
    "                             epoch)\n",
    "            self._validateModel(model,\n",
    "                                epoch)\n",
    "        self._notifiyPostTaught()\n",
    "\n",
    "    def _notifiyPreTeach(self, model, epochs):\n",
    "        self._trainingSubscriber.preModelTeach(model, epochs)\n",
    "        self._validationSubscriber.preModelTeach(model, epochs)\n",
    "\n",
    "    def _notifiyPostTaught(self):\n",
    "        self._trainingSubscriber.postModelTeach()\n",
    "        self._validationSubscriber.postModelTeach()\n",
    "\n",
    "    def _trainModel(self, model, epoch):\n",
    "        self._processData(model,\n",
    "                          self._dataBunch.trainingDataSet,\n",
    "                          epoch,\n",
    "                          self._trainingSubscriber)\n",
    "\n",
    "    def _validateModel(self, model, epoch):\n",
    "        with torch.no_grad():\n",
    "            self._processData(model,\n",
    "                              self._dataBunch.validationDataSet,\n",
    "                              epoch,\n",
    "                              self._validationSubscriber)\n",
    "\n",
    "    def _processData(self,\n",
    "                     model,\n",
    "                     dataLoader,\n",
    "                     epoch,\n",
    "                     processingSubscriber: Subscriber):\n",
    "        processingSubscriber.preEpoch(epoch, dataLoader)\n",
    "        try:\n",
    "            for _xDataBatch, _yDataBatch in dataLoader:\n",
    "                processingSubscriber.preBatchEvaluation()\n",
    "                _predictions = model(_xDataBatch)\n",
    "                processingSubscriber.postBatchEvaluation(_predictions, _yDataBatch)\n",
    "        except ProcessCancellationException: pass\n",
    "        finally:\n",
    "            processingSubscriber.postEpoch(epoch)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#export\n",
    "def createCosineScheduler(start, high, end):\n",
    "    return [cosineScheduler(start, high), cosineScheduler(high, end)]\n",
    "\n",
    "def createWeightAndBiasSchedulers():\n",
    "    phases = [0.3, 0.7]\n",
    "    _weightsScheduler = aggregateSchedulers(phases, createCosineScheduler(0.3, 0.6, 0.2)) \n",
    "    _biasScheduler = aggregateSchedulers(phases, createCosineScheduler(0.9, 1.8, 0.6))\n",
    "    return _weightsScheduler, _biasScheduler"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "validationSubscriber = ValidationSubscriber()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "weightScheduler, biasScheduler = createWeightAndBiasSchedulers()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainingSubscriber = TrainingSubscriber(schedulingFunction=weightScheduler)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "teacher = TeacherEnhanced(imageDataBunch, \n",
    "                          trainingSubscriber,\n",
    "                          validationSubscriber\n",
    "                         )\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "convolutionalModelSR1 = createBetterConvolutionModel(numberOfClasses, layerSizes)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "accuracy(convolutionalModelSR1(validationDataSet.xVector), validationDataSet.yVector)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "teacher.teachModel(convolutionalModelSR1, 3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "accuracy(convolutionalModelSR1(validationDataSet.xVector), validationDataSet.yVector)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainingSubscriber.plotLearningRate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "l = list(map(lambda x: weightScheduler(x/5000), list(range(5000))))\n",
    "plotter.plot(l)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Todo: \n",
    "\n",
    "\n",
    "## Things to look at later\n",
    "\n",
    "- Learning Rate Capping\n",
    "- Bias Learning rate annealing\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}