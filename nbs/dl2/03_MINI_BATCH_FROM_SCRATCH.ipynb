{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from exp.nb_02 import *\n",
    "import torch.nn.functional as Functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "mathPlotLib.rcParams['image.cmap'] = 'plasma'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Setup\n",
    "---\n",
    "\n",
    "- Make Sure Data is normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTraining,yTraining,xValidation,yValidation = getMnistData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that the data is normalized, so that it has $\\mu$ = 0 and $\\sigma$ = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1304), tensor(0.3073))"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xTraining.mean(), xTraining.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Totally not normalized, let's fix that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrainingNormalized = normalizeVector(xTraining, xTraining.mean(), xTraining.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-7.6999e-06), tensor(1.))"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xTrainingNormalized.mean(), xTrainingNormalized.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \\\\0/ \n",
    "Yay! Normalized!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Setup\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, columns = xTrainingNormalized.shape\n",
    "numberOfClasses = yTraining.max() + 1. # Largest digit to recognize :)\n",
    "layerOutput = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numberOfClasses.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitRecognitionModel(torch.nn.Module):\n",
    "    def __init__(self, inputSize, numberOfClasses, layerOutput):\n",
    "        super().__init__()\n",
    "        self.layers = [torch.nn.Linear(inputSize, layerOutput), # Take in image vectors, output reduced matrix\n",
    "                       torch.nn.ReLU(), # Perform activations\n",
    "                       torch.nn.Linear(layerOutput, numberOfClasses)] # Predict digit classes.\n",
    "#         self.layers = [layer.cuda() for layer in self.layers]\n",
    "    def __call__(self, trainingSet):\n",
    "        layerInput = trainingSet\n",
    "        for layer in self.layers: layerInput = layer(layerInput)\n",
    "        return layerInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "digitReconModel = DigitRecognitionModel(columns, numberOfClasses.item(), layerOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Linear(in_features=784, out_features=50, bias=True),\n",
       " ReLU(),\n",
       " Linear(in_features=50, out_features=10, bias=True)]"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digitReconModel.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawPredictions = digitReconModel(xTrainingNormalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 10])"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawPredictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3872, -0.5327, -0.4959,  ..., -0.1437,  0.2116,  0.4025],\n",
       "        [-0.2054, -0.4431, -0.3664,  ..., -0.3712,  0.2399,  0.4051],\n",
       "        [ 0.1426, -0.0157,  0.1514,  ..., -0.0771,  0.2230, -0.0040],\n",
       "        ...,\n",
       "        [ 0.0499, -0.0287, -0.3104,  ..., -0.1593,  0.2385,  0.1238],\n",
       "        [ 0.0585, -0.1225, -0.5098,  ..., -0.0896,  0.0994,  0.4788],\n",
       "        [ 0.3179,  0.1188, -0.0957,  ..., -0.2139,  0.3013,  0.2271]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawPredictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Function\n",
    "---\n",
    "\n",
    "### Predictions\n",
    "\n",
    "Your model has to predict something so that we cat teach it something.\n",
    "\n",
    "First, we will need to compute the softmax of our **activations**. This is defined by:\n",
    "\n",
    "$$\\hbox{softmax}(\\vec{x})_{i} = \\frac{e^{x_{i}}}{\\sum_{0 \\leq j \\leq n-1} e^{x_{j}}}$$ \n",
    "\n",
    "Which inturn turns the activations from the last layer into predictions of the probablity of belonging to one of the output classes.\n",
    "\n",
    "$\\hbox{exp}(constant) = e^{constant}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softMaxLoss(predictionVector):\n",
    "    return (predictionVector.exp()/predictionVector.exp().sum(-1, keepdim=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = softMaxLoss(rawPredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0647, 0.0560, 0.0581,  ..., 0.0826, 0.1178, 0.1426],\n",
       "        [0.0831, 0.0655, 0.0707,  ..., 0.0704, 0.1297, 0.1530],\n",
       "        [0.1082, 0.0923, 0.1091,  ..., 0.0868, 0.1172, 0.0934],\n",
       "        ...,\n",
       "        [0.0985, 0.0911, 0.0687,  ..., 0.0799, 0.1190, 0.1061],\n",
       "        [0.1013, 0.0845, 0.0574,  ..., 0.0873, 0.1055, 0.1542],\n",
       "        [0.1242, 0.1018, 0.0822,  ..., 0.0730, 0.1222, 0.1134]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "       grad_fn=<SumBackward2>)"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.sum(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that it outputs a bunch of ones when we sum the softmax vectors together, which is expected :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Apparently in practice you have to take the log of soft max to get something useful*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicitonsLogged = predictions.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-23.6066, -23.4864, -23.0930,  ..., -23.3039, -23.3799, -23.1664],\n",
       "       grad_fn=<SumBackward2>)"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicitonsLogged.sum(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss\n",
    "### Entropy Loss\n",
    "\n",
    "$$ -\\sum x\\, \\log p(x) $$\n",
    "\n",
    "\n",
    "**Step One: Figure out how to get validation data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 8, 3])"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yTraining[96:99]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the digits that each x is supposed to represent lets see!\n",
    "\n",
    "The 96th is supposed to be a **7**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a2e96fdd8>"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotter.imshow(xTrainingNormalized[96].cpu().view(28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well would you look at that, ain't that neat?\n",
    "\n",
    "Now let's see what our model thought how much this image looked like a **7**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.3281, grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicitonsLogged[96][6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¤”\n",
    "Looks like it did not do a very good job, let's tell it what it needs to do to fix that\n",
    "\n",
    "First, let's look at what our range looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.4033, -2.1348, -2.5309, -2.2711, -2.6368, -2.1228, -2.3281, -2.4963,\n",
       "         -2.2511, -2.0242],\n",
       "        [-2.5394, -2.5198, -2.9830, -1.9815, -2.4293, -2.2612, -2.4765, -2.7058,\n",
       "         -1.9408, -1.7998],\n",
       "        [-2.2056, -2.2809, -2.6385, -2.1681, -2.4669, -2.3054, -2.0655, -2.5011,\n",
       "         -2.1394, -2.3999]], grad_fn=<IndexBackward>)"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicitonsLogged[[96,97,98]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's remind ourselves what our expected digits are suppossed to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 8, 3])"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yTraining[96:99]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combine all for profit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.4963, -1.9408, -2.1681], grad_fn=<IndexBackward>)"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicitonsLogged[[96,97,98], [7,8,3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see what our model's predictions where for each of the digits. And **they are all bad**!\n",
    "\n",
    "Let's now use this knowlege to perform\n",
    "\n",
    "$$ -\\sum x\\, \\log p(x) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFundie(predictionVector, expectedVector): # expected vector is an array of numbers eg [4, 3, 9, 8] which correspond to each of the output classes.\n",
    "    items = range(predictionVector.shape[0]) # 0 to 50000 iterator\n",
    "    return -predictionVector[items, expectedVector].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = lossFundie(predicitonsLogged, yTraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3218, grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enhanced LogMax!\n",
    "\n",
    "Then, there is a way to compute the log of the sum of exponentials in a more stable way, called the [LogSumExp trick](https://en.wikipedia.org/wiki/LogSumExp). The idea is to use the following formula:\n",
    "\n",
    "$$\\log \\left ( \\sum_{j=1}^{n} e^{x_{j}} \\right ) = \\log \\left ( e^{a} \\sum_{j=1}^{n} e^{x_{j}-a} \\right ) = a + \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}-a} \\right )$$\n",
    "\n",
    "where a is the maximum of the $x_{j}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logMaxEnhanced(predictions):\n",
    "    return predictions - predictions.logsumexp(-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsEnhanced = logMaxEnhanced(predicitonsLogged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "testNear(lossFundie(predictionsEnhanced, yTraining), loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all of the things that we need to tell our model that it is doing a bad job. Now let's move on to teaching it how to be better. The first step there is the \n",
    "\n",
    "Training loop\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "realLossFunction = Functional.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def accuracy(predictionVector, expectedVector):\n",
    "    return (torch.argmax(predictionVector, dim=1) == expectedVector).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputBatch = xTrainingNormalized[0:batchSize]\n",
    "expectedOutputBatch = yTraining[0:batchSize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = digitReconModel(inputBatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 10]),\n",
       " tensor([-0.3872, -0.5327, -0.4959,  0.3915, -0.0376,  0.3730,  0.1169, -0.1437,\n",
       "          0.2116,  0.4025], grad_fn=<SelectBackward>))"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape, predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsSoftMaxed = softMaxLoss(predictions).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 9, 8,  ..., 6, 9, 0])"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The numbers our model guessed\n",
    "torch.argmax(predictionsEnhanced, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.9480, -1.8775, -2.1438, -2.0242, -2.0767, -1.7691, -1.9394, -1.8847,\n",
       "        -1.9641, -2.0388, -2.0140, -2.0925, -2.0549, -2.0589, -2.0535, -1.9882,\n",
       "        -1.7559, -1.9391, -2.1218, -2.1305, -2.0132, -1.9379, -2.1349, -2.0537,\n",
       "        -1.5373, -1.7949, -2.0526, -1.9774, -1.7432, -2.0310, -2.0379, -1.7661,\n",
       "        -1.8911, -1.9774, -1.8800, -2.0218, -1.9286, -1.9246, -1.9899, -1.8869,\n",
       "        -2.1555, -1.8745, -2.0541, -2.0687, -2.0345, -1.8872, -2.0867, -1.9217,\n",
       "        -2.0325, -1.9956, -1.9493, -1.9070, -2.0198, -2.0715, -1.9224, -1.9747,\n",
       "        -1.9991, -2.1033, -1.9085, -2.0083, -2.1535, -1.9370, -2.0715, -1.9119],\n",
       "       grad_fn=<IndexBackward>)"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The largest prediction levels for each tuple.\n",
    "predictionsSoftMaxed[range(0, batchSize), torch.argmax(predictionsSoftMaxed, dim=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1250)"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(predictions, expectedOutputBatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About as good as random guessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "learningRate = 0.5\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get ready to train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range((rows//batchSize + 1)): # Process each of the batches\n",
    "        beginningRange = i * batchSize\n",
    "        endingRange = beginningRange + batchSize\n",
    "        _inputBatch = xTrainingNormalized[beginningRange:endingRange]\n",
    "        validationBatch = yTraining[beginningRange:endingRange]\n",
    "        preds = digitReconModel(_inputBatch)\n",
    "        loss = realLossFunction(preds, validationBatch)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for layer in digitReconModel.layers:\n",
    "                if hasattr(layer, 'weight'):\n",
    "                    layer.weight -= layer.weight.grad * learningRate\n",
    "                    layer.bias -= layer.bias.grad * learningRate\n",
    "                    layer.weight.grad.zero_()\n",
    "                    layer.bias.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1,\n",
       "        1, 3, 4, 3, 7, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7, 6, 1, 8, 7, 9, 3, 9, 8, 5,\n",
       "        5, 3, 3, 0, 7, 4, 9, 8, 0, 9, 4, 1, 4, 4, 6, 0])"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(digitReconModel(inputBatch), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1,\n",
       "        1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7, 6, 1, 8, 7, 9, 3, 9, 8, 5,\n",
       "        9, 3, 3, 0, 7, 4, 9, 8, 0, 9, 4, 1, 4, 4, 6, 0])"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expectedOutputBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9355)"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(digitReconModel(xTrainingNormalized[69:100]), yTraining[69:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay! Higher accuracy rates!\n",
    "\n",
    "**Moving on to refactoring the model to have that enhancement**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedDigitModel(torch.nn.Module): \n",
    "    def __init__(self, inputSize, numberHiddenLayers, classes, learningRate):\n",
    "        super().__init__()\n",
    "        self.layerOne = torch.nn.Linear(inputSize, numberHiddenLayers)\n",
    "        self.layerTwo = torch.nn.Linear(numberHiddenLayers, classes)\n",
    "        self.learningRate = learningRate\n",
    "        \n",
    "    def __call__(self, inputMatrix): return self.layerTwo(Functional.relu(self.layerOne(inputMatrix)))\n",
    "    \n",
    "    def zeroOutGradients(self):\n",
    "        for layerParameter in self.parameters : layerParameter.grad.data.zero_()\n",
    "    \n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for layerParameter in self.parameters : layerParameter -= layerParameter.grad * self.learningRate\n",
    "    \n",
    "    def learn(self):\n",
    "        self.zeroOutGradients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhancedDigitModel = EnhancedDigitModel(columns, layerOutput, numberOfClasses.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = enhancedDigitModel(inputBatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 8, 0, 2, 8, 8, 2, 2, 8, 2, 2, 2, 2, 6, 8, 2, 6, 2, 3, 2, 2, 2, 2, 2,\n",
       "        2, 8, 8, 2, 2, 8, 2, 2, 6, 2, 8, 2, 2, 8, 8, 8, 8, 8, 8, 2, 2, 8, 2, 8,\n",
       "        9, 2, 6, 2, 8, 3, 2, 2, 8, 8, 2, 2, 8, 8, 3, 2])"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(predictions, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1,\n",
       "        1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7, 6, 1, 8, 7, 9, 3, 9, 8, 5,\n",
       "        9, 3, 3, 0, 7, 4, 9, 8, 0, 9, 4, 1, 4, 4, 6, 0])"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expectedOutputBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0781)"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(predictions, expectedOutputBatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "not very good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnhancedDigitModel(\n",
       "  (layerOne): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (layerTwo): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enhancedDigitModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range((rows//batchSize + 1)): # Process each of the batches\n",
    "        beginningRange = i * batchSize\n",
    "        endingRange = beginningRange + batchSize\n",
    "        _inputBatch = xTrainingNormalized[beginningRange:endingRange]\n",
    "        validationBatch = yTraining[beginningRange:endingRange]\n",
    "        preds = enhancedDigitModel(_inputBatch)\n",
    "        loss = realLossFunction(preds, validationBatch)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for layerParameter in enhancedDigitModel.parameters(): \n",
    "                layerParameter -= layerParameter.grad * learningRate\n",
    "            enhancedDigitModel.zero_grad() # method inherited from the nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9375)"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(enhancedDigitModel(inputBatch), expectedOutputBatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfLearningModel(torch.nn.Module): \n",
    "    def __init__(self, inputSize, numberHiddenLayers, classes, learningRate):\n",
    "        super().__init__()\n",
    "        self.layerOne = torch.nn.Linear(inputSize, numberHiddenLayers)\n",
    "        self.layerTwo = torch.nn.Linear(numberHiddenLayers, classes)\n",
    "        self.learningRate = learningRate\n",
    "        \n",
    "    def __call__(self, inputMatrix): return self.layerTwo(Functional.relu(self.layerOne(inputMatrix)))\n",
    "    \n",
    "    def zeroOutGradients(self):\n",
    "        for layerParameter in self.parameters() : layerParameter.grad.data.zero_()\n",
    "    \n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for layerParameter in self.parameters() : layerParameter -= layerParameter.grad * self.learningRate\n",
    "    \n",
    "    def learn(self):\n",
    "        self.step()\n",
    "        self.zeroOutGradients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchData(iteration):\n",
    "    beginningRange = iteration * batchSize\n",
    "    endingRange = beginningRange + batchSize\n",
    "    _inputBatch = xTrainingNormalized[beginningRange:endingRange]\n",
    "    validationBatch = yTraining[beginningRange:endingRange]\n",
    "    return _inputBatch, validationBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(learningModel):\n",
    "    for epoch in range(epochs):\n",
    "        for i in range((rows)//batchSize + 1):\n",
    "            _inputBatch, _expectedBatch = batchData(i)\n",
    "            _predictions = learningModel(_inputBatch)\n",
    "            loss = realLossFunction(_predictions, _expectedBatch)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            learningModel.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "selfLearner = SelfLearningModel(columns, layerOutput, numberOfClasses.item(), learningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0938)"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(selfLearner(inputBatch), expectedOutputBatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainModel(selfLearner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8281)"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newPred = selfLearner(inputBatch)\n",
    "accuracy(newPred, expectedOutputBatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a26a954a8>"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotter.imshow(inputBatch[5].view(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(newPred[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfLearningLibraryModel(torch.nn.Module): \n",
    "    def __init__(self, inputSize, numberHiddenLayers, classes, learningRate):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(inputSize, numberHiddenLayers),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(numberHiddenLayers, classes)\n",
    "        )\n",
    "        self.optimizer = optim.SGD(self.parameters(), learningRate)\n",
    "        \n",
    "    def __call__(self, inputMatrix): return self.layers(inputMatrix)\n",
    "    \n",
    "    def learn(self):\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "librarySelfLearner = SelfLearningLibraryModel(columns, layerOutput, numberOfClasses.item(), learningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "libraryPreds = librarySelfLearner(inputBatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0156)"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(libraryPreds, expectedOutputBatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainModel(librarySelfLearner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9531)"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(librarySelfLearner(inputBatch), expectedOutputBatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Dataset:\n",
    "    def __init__(self, xVector, yVector):\n",
    "        self.xVector = xVector\n",
    "        self.yVector = yVector\n",
    "    def __len__(self): return len(self.xVector)\n",
    "    def __getitem__(self, i): return self.xVector[i],self.yVector[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "xValidationNormalized = normalizeVector(xValidation, xValidation.mean(), xValidation.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingDataset, validationDataSet = Dataset(xTrainingNormalized, yTraining), Dataset(xValidation, yValidation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4)"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingDataset[2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(trainingDataset) == len(xTrainingNormalized)\n",
    "assert len(validationDataSet) == len(xValidationNormalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "xData, yData = trainingDataset[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert xData.shape == (5, 28*28)\n",
    "assert yData.shape == (5,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, dataSet, batchSize):\n",
    "        self.dataSet = dataSet\n",
    "        self.batchSize = batchSize\n",
    "    def __iter__(self):\n",
    "        for i in range(0, len(self.dataSet), self.batchSize): yield self.dataSet[i:i+self.batchSize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModelEnhanced(learnableModel, dataLoader, validationDataLoader):\n",
    "    for epoch in range(epochs):\n",
    "        for _xTrain, _yTrain in dataLoader:\n",
    "            _preds = learnableModel(_xTrain)\n",
    "            loss = realLossFunction(_preds, _yTrain)\n",
    "            loss.backward()\n",
    "            learnableModel.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelToTeach = SelfLearningLibraryModel(columns, layerOutput, numberOfClasses.item(), learningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0625)"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(modelToTeach(inputBatch), expectedOutputBatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainModelEnhanced(modelToTeach, DataLoader(trainingDataset, batchSize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9062)"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(modelToTeach(inputBatch), expectedOutputBatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8933)"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(modelToTeach(xValidationNormalized), yValidation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Sampling\n",
    "\n",
    "The enhanced data loader that shuffles the data (and also keeps the labels correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomSampler:\n",
    "    def __init__(self, numberOfItems, batchSize, shuffle=True):\n",
    "        self.numberOfItems, self.batchSize, self.shuffle = numberOfItems, batchSize, shuffle\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.shuffledIndices = torch.randperm(self.numberOfItems) if self.shuffle else torch.arange(self.numberOfItems)\n",
    "        for batch in range(0, self.numberOfItems, self.batchSize): yield self.shuffledIndices[batch: batch + self.batchSize]\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([3, 0]), tensor([6, 2]), tensor([1, 4]), tensor([5, 7])]"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rando = RandomSampler(8, 2)\n",
    "[batch for batch in rando]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0, 1]), tensor([2, 3]), tensor([4, 5]), tensor([6, 7]), tensor([8])]"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rando = RandomSampler(9, 2, False)\n",
    "[batch for batch in rando]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "things = [\n",
    "    \"One\",\n",
    "    \"Two\",\n",
    "    \"Three\",\n",
    "    \"Four\",\n",
    "    \"Five\",\n",
    "    \"Six\",\n",
    "    \"Seven\",\n",
    "    \"Eight\",\n",
    "    \"Nine\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Two', 'Four'], ['Six', 'Eight'], ['Seven', 'One'], ['Five', 'Three']]"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rando = RandomSampler(8, 2)\n",
    "[ list(map(lambda i: things[i], _batch)) for _batch in rando]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collateTuple(tupl):\n",
    "    xColumn, yColumn = zip(*tupl)\n",
    "    return torch.stack(xColumn), torch.stack(yColumn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedDataLoader:\n",
    "    def __init__(self, dataSet, batchSize):\n",
    "        self.dataSet = dataSet\n",
    "        self.batchSize = batchSize\n",
    "        self.sampler = RandomSampler(len(dataSet), batchSize)\n",
    "    def __iter__(self):\n",
    "        for _batch in self.sampler: yield collateTuple([ self.dataSet[_index] for _index in _batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "testLoader = EnhancedDataLoader(things, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-600-47b7fd6958bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0m_thing\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtestLoader\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_thing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-576-202329238b16>\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataSet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0m_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32myield\u001b[0m \u001b[0mcollateTuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataSet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-599-fc0601719fcc>\u001b[0m in \u001b[0;36mcollateTuple\u001b[0;34m(tupl)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcollateTuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtupl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mxColumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myColumn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtupl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "for _thing in testLoader: print(_thing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "anotherTestLoader = EnhancedDataLoader(Dataset(inputBatch, expectedOutputBatch), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10)\n",
      "(10, 10)\n",
      "(10, 10)\n",
      "(10, 10)\n",
      "(10, 10)\n",
      "(10, 10)\n",
      "(4, 4)\n"
     ]
    }
   ],
   "source": [
    "for _thing in anotherTestLoader: \n",
    "    a, b =_thing \n",
    "    print((len(a), len(b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDigitModel = SelfLearningLibraryModel(columns, layerOutput, numberOfClasses.item(), learningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0625)"
      ]
     },
     "execution_count": 608,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(newDigitModel(inputBatch), expectedOutputBatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainModelEnhanced(newDigitModel, EnhancedDataLoader(Dataset(xTrainingNormalized, yTraining), 75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9375)"
      ]
     },
     "execution_count": 612,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(newDigitModel(inputBatch), expectedOutputBatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries Data Stuff ._."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "brandNewDigitModel = SelfLearningLibraryModel(columns, layerOutput, numberOfClasses.item(), learningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0469)"
      ]
     },
     "execution_count": 614,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(brandNewDigitModel(inputBatch), expectedOutputBatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainModelEnhanced(brandNewDigitModel, DataLoader(collate_fn=collateTuple, \n",
    "                                                  dataset=Dataset(xTrainingNormalized, yTraining), \n",
    "                                                  batch_size=75,\n",
    "                                                  shuffle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9531)"
      ]
     },
     "execution_count": 618,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(brandNewDigitModel(inputBatch), expectedOutputBatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9445)"
      ]
     },
     "execution_count": 619,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(brandNewDigitModel(xValidationNormalized), yValidation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Better Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModelWithValidation(learnableModel, dataLoader, validationDataLoader, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for _xTrain, _yTrain in dataLoader:\n",
    "            _preds = learnableModel(_xTrain)\n",
    "            loss = realLossFunction(_preds, _yTrain)\n",
    "            loss.backward()\n",
    "            learnableModel.learn()\n",
    "        accumulatedLoss, accumulatedAccuracy = 0.,0.\n",
    "        for _xValidation, _yValidation in validationDataLoader:\n",
    "            with torch.no_grad(): # do not produce gradients when running loss function\n",
    "                _preds = learnableModel(_xValidation)\n",
    "                accumulatedLoss += realLossFunction(_preds, _yValidation)\n",
    "                accumulatedAccuracy += accuracy(_preds, _yValidation)\n",
    "        numberOfItems = len(validationDataLoader)\n",
    "        print(\"Epoch {}, Accuracy {}, Loss {}\".format(epoch, accumulatedAccuracy/numberOfItems, accumulatedLoss/numberOfItems))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [],
   "source": [
    "fancyDigitModel = SelfLearningLibraryModel(columns, layerOutput, numberOfClasses.item(), learningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1250)"
      ]
     },
     "execution_count": 637,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(fancyDigitModel(inputBatch), expectedOutputBatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Accuracy 0.9192036390304565, Loss 0.2772819995880127\n",
      "Epoch 1, Accuracy 0.9257708191871643, Loss 0.28181517124176025\n",
      "Epoch 2, Accuracy 0.9156214594841003, Loss 0.30584725737571716\n",
      "Epoch 3, Accuracy 0.9494523406028748, Loss 0.18209196627140045\n",
      "Epoch 4, Accuracy 0.9541289210319519, Loss 0.1749899834394455\n"
     ]
    }
   ],
   "source": [
    "trainModelWithValidation(fancyDigitModel, \n",
    "                         DataLoader(collate_fn=collateTuple, \n",
    "                                                  dataset=Dataset(xTrainingNormalized, yTraining), \n",
    "                                                  batch_size=75,\n",
    "                                                  shuffle=True), \n",
    "                         DataLoader(collate_fn=collateTuple, \n",
    "                                                  dataset=Dataset(xValidationNormalized, yValidation), \n",
    "                                                  batch_size=75), \n",
    "                         5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9505)"
      ]
     },
     "execution_count": 634,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(fancyDigitModel(xValidationNormalized), yValidation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def createDataLoaders(trainingDataSet, validationDataSet, batchSize, **kwargs):\n",
    "    return (\n",
    "        DataLoader(trainingDataSet, batch_size=batchSize, shuffle=True, **kwargs),\n",
    "        DataLoader(validationDataSet, batch_size=batchSize*2, **kwargs)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDigitModel = SelfLearningLibraryModel(columns, layerOutput, numberOfClasses.item(), learningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingDataLoader, validationDataLoader = createDataLoaders(Dataset(xTrainingNormalized, yTraining), \n",
    "                                                             Dataset(xValidationNormalized, yValidation), \n",
    "                                                             batchSize, \n",
    "                                                             collate_fn=collateTuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0997)"
      ]
     },
     "execution_count": 657,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(testDigitModel(xValidationNormalized), yValidation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Accuracy 0.8950751423835754, Loss 0.4031490981578827\n",
      "Epoch 1, Accuracy 0.804094135761261, Loss 0.8487293124198914\n",
      "Epoch 2, Accuracy 0.9528283476829529, Loss 0.1820055991411209\n",
      "Epoch 3, Accuracy 0.8481012582778931, Loss 1.1656832695007324\n",
      "Epoch 4, Accuracy 0.9336432218551636, Loss 0.32268327474594116\n",
      "Epoch 5, Accuracy 0.9587618708610535, Loss 0.18828605115413666\n",
      "Epoch 6, Accuracy 0.962717592716217, Loss 0.15694169700145721\n"
     ]
    }
   ],
   "source": [
    "trainModelWithValidation(testDigitModel, trainingDataLoader, validationDataLoader, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9623)"
      ]
     },
     "execution_count": 659,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(testDigitModel(xValidationNormalized), yValidation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"notebook2script.py\", line 3, in <module>\n",
      "    import json,fire,re\n",
      "ModuleNotFoundError: No module named 'fire'\n"
     ]
    }
   ],
   "source": [
    "!python notebook2script.py 03_MINI_BATCH_FROM_SCRATCH.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
