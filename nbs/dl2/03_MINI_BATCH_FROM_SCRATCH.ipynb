{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from exp.nb_02 import *\n",
    "import torch.nn.functional as Functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mathPlotLib.rcParams['image.cmap'] = 'plasma'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Setup\n",
    "---\n",
    "\n",
    "- Make Sure Data is normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTraining,yTraining,xValidation,yValidation = getMnistData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that the data is normalized, so that it has $\\mu$ = 0 and $\\sigma$ = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1304), tensor(0.3073))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xTraining.mean(), xTraining.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Totally not normalized, let's fix that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrainingNormalized = normalizeVector(xTraining, xTraining.mean(), xTraining.std()).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(9.9845e-05, device='cuda:0'), tensor(1.0000, device='cuda:0'))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xTrainingNormalized.mean(), xTrainingNormalized.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \\\\0/ \n",
    "Yay! Normalized!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Setup\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, columns = xTrainingNormalized.shape\n",
    "numberOfClasses = yTraining.max() + 1. # Largest digit to recognize :)\n",
    "layerOutput = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numberOfClasses.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitRecognitionModel(torch.nn.Module):\n",
    "    def __init__(self, inputSize, numberOfClasses, layerOutput):\n",
    "        super().__init__()\n",
    "        self.layers = [torch.nn.Linear(inputSize, layerOutput), # Take in image vectors, output reduced matrix\n",
    "                       torch.nn.ReLU(), # Perform activations\n",
    "                       torch.nn.Linear(layerOutput, numberOfClasses)] # Predict digit classes.\n",
    "        self.layers = [layer.cuda() for layer in self.layers]\n",
    "    def __call__(self, trainingSet):\n",
    "        layerInput = trainingSet\n",
    "        for layer in self.layers: layerInput = layer(layerInput)\n",
    "        return layerInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "digitReconModel = DigitRecognitionModel(columns, numberOfClasses.item(), layerOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Linear(in_features=784, out_features=75, bias=True),\n",
       " ReLU(),\n",
       " Linear(in_features=75, out_features=10, bias=True)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digitReconModel.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawPredictions = digitReconModel(xTrainingNormalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 10])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawPredictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2421,  0.2581,  0.4019,  ...,  0.4451,  0.0326, -0.0482],\n",
       "        [-0.2083,  0.1755, -0.1983,  ...,  0.3915,  0.1221, -0.0784],\n",
       "        [ 0.2063,  0.1454,  0.1402,  ...,  0.1910, -0.2170, -0.1771],\n",
       "        ...,\n",
       "        [-0.0745, -0.0070,  0.1895,  ...,  0.4792,  0.2938, -0.2122],\n",
       "        [-0.1981, -0.2195,  0.2124,  ...,  0.4679,  0.0144, -0.0259],\n",
       "        [-0.1351, -0.1892,  0.0784,  ...,  0.0853,  0.0184, -0.1992]],\n",
       "       device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawPredictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Function\n",
    "---\n",
    "\n",
    "### Predictions\n",
    "\n",
    "Your model has to predict something so that we cat teach it something.\n",
    "\n",
    "First, we will need to compute the softmax of our **activations**. This is defined by:\n",
    "\n",
    "$$\\hbox{softmax}(\\vec{x})_{i} = \\frac{e^{x_{i}}}{\\sum_{0 \\leq j \\leq n-1} e^{x_{j}}}$$ \n",
    "\n",
    "Which inturn turns the activations from the last layer into predictions of the probablity of belonging to one of the output classes.\n",
    "\n",
    "$\\hbox{exp}(constant) = e^{constant}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softMaxLoss(predictionVector):\n",
    "    return (predictionVector.exp()/predictionVector.exp().sum(-1, keepdim=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = softMaxLoss(rawPredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0711, 0.1172, 0.1354,  ..., 0.1414, 0.0936, 0.0863],\n",
       "        [0.0754, 0.1107, 0.0762,  ..., 0.1374, 0.1050, 0.0859],\n",
       "        [0.1123, 0.1057, 0.1051,  ..., 0.1106, 0.0735, 0.0765],\n",
       "        ...,\n",
       "        [0.0843, 0.0902, 0.1098,  ..., 0.1467, 0.1219, 0.0735],\n",
       "        [0.0782, 0.0765, 0.1178,  ..., 0.1521, 0.0967, 0.0928],\n",
       "        [0.0850, 0.0806, 0.1053,  ..., 0.1060, 0.0991, 0.0797]],\n",
       "       device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000], device='cuda:0',\n",
       "       grad_fn=<SumBackward2>)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.sum(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that it outputs a bunch of ones when we sum the softmax vectors together, which is expected :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Apparently in practice you have to take the log of soft max to get something useful*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicitonsLogged = predictions.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-23.2805, -23.2267, -23.1809,  ..., -23.2632, -23.2295, -23.1738],\n",
       "       device='cuda:0', grad_fn=<SumBackward2>)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicitonsLogged.sum(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss\n",
    "### Entropy Loss\n",
    "\n",
    "$$ -\\sum x\\, \\log p(x) $$\n",
    "\n",
    "\n",
    "**Step One: Figure out how to get validation data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 8, 3])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yTraining[96:99]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the digits that each x is supposed to represent lets see!\n",
    "\n",
    "The 96th is supposed to be a **7**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f101f1d1518>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotter.imshow(xTrainingNormalized[96].cpu().view(28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well would you look at that, ain't that neat?\n",
    "\n",
    "Now let's see what our model thought how much this image looked like a **7**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.4450, device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicitonsLogged[96][6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¤”\n",
    "Looks like it did not do a very good job, let's tell it what it needs to do to fix that\n",
    "\n",
    "First, let's look at what our range looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predicitonsLogged[[96,97,98]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's remind ourselves what our expected digits are suppossed to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 8, 3])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yTraining[96:99]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combine all for profit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.0879, -2.2056, -2.3139], device='cuda:0', grad_fn=<IndexBackward>)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicitonsLogged[[96,97,98], [7,8,3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see what our model's predictions where for each of the digits. And they are all bad!\n",
    "\n",
    "Let's now use this knowlege to perform\n",
    "\n",
    "$$ -\\sum x\\, \\log p(x) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFundie(predictionVector, expectedVector): # expected vector is an array of numbers eg [4, 3, 9, 8] which correspond to each of the output classes.\n",
    "    items = range(predictionVector.shape[0]) # 0 to 50000 iterator\n",
    "    return -predictionVector[items, expectedVector].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = lossFundie(predicitonsLogged, yTraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3454, device='cuda:0', grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enhanced LogMax!\n",
    "\n",
    "Then, there is a way to compute the log of the sum of exponentials in a more stable way, called the [LogSumExp trick](https://en.wikipedia.org/wiki/LogSumExp). The idea is to use the following formula:\n",
    "\n",
    "$$\\log \\left ( \\sum_{j=1}^{n} e^{x_{j}} \\right ) = \\log \\left ( e^{a} \\sum_{j=1}^{n} e^{x_{j}-a} \\right ) = a + \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}-a} \\right )$$\n",
    "\n",
    "where a is the maximum of the $x_{j}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logMaxEnhanced(predictions):\n",
    "    return predictions - predictions.logsumexp(-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsEnhanced = logMaxEnhanced(predicitonsLogged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "near:\ntensor([[-2.6436, -2.1435, -1.9996,  ..., -1.9564, -2.3689, -2.4498],\n        [-2.5844, -2.2007, -2.5744,  ..., -1.9847, -2.2541, -2.4546],\n        [-2.1866, -2.2475, -2.2527,  ..., -2.2018, -2.6099, -2.5700],\n        ...,\n        [-2.4728, -2.4054, -2.2089,  ..., -1.9192, -2.1046, -2.6105],\n        [-2.5490, -2.5704, -2.1385,  ..., -1.8830, -2.3365, -2.3768],\n        [-2.4647, -2.5189, -2.2513,  ..., -2.2444, -2.3113, -2.5289]],\n       device='cuda:0', grad_fn=<SubBackward0>)\ntensor([[0.0711, 0.1172, 0.1354,  ..., 0.1414, 0.0936, 0.0863],\n        [0.0754, 0.1107, 0.0762,  ..., 0.1374, 0.1050, 0.0859],\n        [0.1123, 0.1057, 0.1051,  ..., 0.1106, 0.0735, 0.0765],\n        ...,\n        [0.0843, 0.0902, 0.1098,  ..., 0.1467, 0.1219, 0.0735],\n        [0.0782, 0.0765, 0.1178,  ..., 0.1521, 0.0967, 0.0928],\n        [0.0850, 0.0806, 0.1053,  ..., 0.1060, 0.0991, 0.0797]],\n       device='cuda:0', grad_fn=<DivBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-148-45095667cc42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtestNear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictionsEnhanced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/course-v3/nbs/dl2/exp/nb_01.py\u001b[0m in \u001b[0;36mtestNear\u001b[0;34m(A, B)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mtestNear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/course-v3/nbs/dl2/exp/nb_01.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(a, b, cmp, cname)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcmp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mcmp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34mf\"{cname}:\\n{a}\\n{b}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_eq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'=='\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: near:\ntensor([[-2.6436, -2.1435, -1.9996,  ..., -1.9564, -2.3689, -2.4498],\n        [-2.5844, -2.2007, -2.5744,  ..., -1.9847, -2.2541, -2.4546],\n        [-2.1866, -2.2475, -2.2527,  ..., -2.2018, -2.6099, -2.5700],\n        ...,\n        [-2.4728, -2.4054, -2.2089,  ..., -1.9192, -2.1046, -2.6105],\n        [-2.5490, -2.5704, -2.1385,  ..., -1.8830, -2.3365, -2.3768],\n        [-2.4647, -2.5189, -2.2513,  ..., -2.2444, -2.3113, -2.5289]],\n       device='cuda:0', grad_fn=<SubBackward0>)\ntensor([[0.0711, 0.1172, 0.1354,  ..., 0.1414, 0.0936, 0.0863],\n        [0.0754, 0.1107, 0.0762,  ..., 0.1374, 0.1050, 0.0859],\n        [0.1123, 0.1057, 0.1051,  ..., 0.1106, 0.0735, 0.0765],\n        ...,\n        [0.0843, 0.0902, 0.1098,  ..., 0.1467, 0.1219, 0.0735],\n        [0.0782, 0.0765, 0.1178,  ..., 0.1521, 0.0967, 0.0928],\n        [0.0850, 0.0806, 0.1053,  ..., 0.1060, 0.0991, 0.0797]],\n       device='cuda:0', grad_fn=<DivBackward0>)"
     ]
    }
   ],
   "source": [
    "testNear(predictionsEnhanced, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
