{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from exp.nb_02 import *\n",
    "import torch.nn.functional as Functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "mathPlotLib.rcParams['image.cmap'] = 'plasma'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Setup\n",
    "---\n",
    "\n",
    "- Make Sure Data is normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTraining,yTraining,xValidation,yValidation = getMnistData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that the data is normalized, so that it has $\\mu$ = 0 and $\\sigma$ = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1304), tensor(0.3073))"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xTraining.mean(), xTraining.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Totally not normalized, let's fix that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrainingNormalized = normalizeVector(xTraining, xTraining.mean(), xTraining.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-7.6999e-06), tensor(1.))"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xTrainingNormalized.mean(), xTrainingNormalized.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \\\\0/ \n",
    "Yay! Normalized!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Setup\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, columns = xTrainingNormalized.shape\n",
    "numberOfClasses = yTraining.max() + 1. # Largest digit to recognize :)\n",
    "layerOutput = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numberOfClasses.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitRecognitionModel(torch.nn.Module):\n",
    "    def __init__(self, inputSize, numberOfClasses, layerOutput):\n",
    "        super().__init__()\n",
    "        self.layers = [torch.nn.Linear(inputSize, layerOutput), # Take in image vectors, output reduced matrix\n",
    "                       torch.nn.ReLU(), # Perform activations\n",
    "                       torch.nn.Linear(layerOutput, numberOfClasses)] # Predict digit classes.\n",
    "#         self.layers = [layer.cuda() for layer in self.layers]\n",
    "    def __call__(self, trainingSet):\n",
    "        layerInput = trainingSet\n",
    "        for layer in self.layers: layerInput = layer(layerInput)\n",
    "        return layerInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "digitReconModel = DigitRecognitionModel(columns, numberOfClasses.item(), layerOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Linear(in_features=784, out_features=50, bias=True),\n",
       " ReLU(),\n",
       " Linear(in_features=50, out_features=10, bias=True)]"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digitReconModel.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawPredictions = digitReconModel(xTrainingNormalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 10])"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawPredictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3872, -0.5327, -0.4959,  ..., -0.1437,  0.2116,  0.4025],\n",
       "        [-0.2054, -0.4431, -0.3664,  ..., -0.3712,  0.2399,  0.4051],\n",
       "        [ 0.1426, -0.0157,  0.1514,  ..., -0.0771,  0.2230, -0.0040],\n",
       "        ...,\n",
       "        [ 0.0499, -0.0287, -0.3104,  ..., -0.1593,  0.2385,  0.1238],\n",
       "        [ 0.0585, -0.1225, -0.5098,  ..., -0.0896,  0.0994,  0.4788],\n",
       "        [ 0.3179,  0.1188, -0.0957,  ..., -0.2139,  0.3013,  0.2271]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawPredictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Function\n",
    "---\n",
    "\n",
    "### Predictions\n",
    "\n",
    "Your model has to predict something so that we cat teach it something.\n",
    "\n",
    "First, we will need to compute the softmax of our **activations**. This is defined by:\n",
    "\n",
    "$$\\hbox{softmax}(\\vec{x})_{i} = \\frac{e^{x_{i}}}{\\sum_{0 \\leq j \\leq n-1} e^{x_{j}}}$$ \n",
    "\n",
    "Which inturn turns the activations from the last layer into predictions of the probablity of belonging to one of the output classes.\n",
    "\n",
    "$\\hbox{exp}(constant) = e^{constant}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softMaxLoss(predictionVector):\n",
    "    return (predictionVector.exp()/predictionVector.exp().sum(-1, keepdim=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = softMaxLoss(rawPredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0647, 0.0560, 0.0581,  ..., 0.0826, 0.1178, 0.1426],\n",
       "        [0.0831, 0.0655, 0.0707,  ..., 0.0704, 0.1297, 0.1530],\n",
       "        [0.1082, 0.0923, 0.1091,  ..., 0.0868, 0.1172, 0.0934],\n",
       "        ...,\n",
       "        [0.0985, 0.0911, 0.0687,  ..., 0.0799, 0.1190, 0.1061],\n",
       "        [0.1013, 0.0845, 0.0574,  ..., 0.0873, 0.1055, 0.1542],\n",
       "        [0.1242, 0.1018, 0.0822,  ..., 0.0730, 0.1222, 0.1134]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "       grad_fn=<SumBackward2>)"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.sum(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that it outputs a bunch of ones when we sum the softmax vectors together, which is expected :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Apparently in practice you have to take the log of soft max to get something useful*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicitonsLogged = predictions.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-23.6066, -23.4864, -23.0930,  ..., -23.3039, -23.3799, -23.1664],\n",
       "       grad_fn=<SumBackward2>)"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicitonsLogged.sum(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss\n",
    "### Entropy Loss\n",
    "\n",
    "$$ -\\sum x\\, \\log p(x) $$\n",
    "\n",
    "\n",
    "**Step One: Figure out how to get validation data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 8, 3])"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yTraining[96:99]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the digits that each x is supposed to represent lets see!\n",
    "\n",
    "The 96th is supposed to be a **7**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a2e96fdd8>"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotter.imshow(xTrainingNormalized[96].cpu().view(28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well would you look at that, ain't that neat?\n",
    "\n",
    "Now let's see what our model thought how much this image looked like a **7**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.3281, grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicitonsLogged[96][6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¤”\n",
    "Looks like it did not do a very good job, let's tell it what it needs to do to fix that\n",
    "\n",
    "First, let's look at what our range looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.4033, -2.1348, -2.5309, -2.2711, -2.6368, -2.1228, -2.3281, -2.4963,\n",
       "         -2.2511, -2.0242],\n",
       "        [-2.5394, -2.5198, -2.9830, -1.9815, -2.4293, -2.2612, -2.4765, -2.7058,\n",
       "         -1.9408, -1.7998],\n",
       "        [-2.2056, -2.2809, -2.6385, -2.1681, -2.4669, -2.3054, -2.0655, -2.5011,\n",
       "         -2.1394, -2.3999]], grad_fn=<IndexBackward>)"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicitonsLogged[[96,97,98]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's remind ourselves what our expected digits are suppossed to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 8, 3])"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yTraining[96:99]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combine all for profit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.4963, -1.9408, -2.1681], grad_fn=<IndexBackward>)"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicitonsLogged[[96,97,98], [7,8,3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see what our model's predictions where for each of the digits. And **they are all bad**!\n",
    "\n",
    "Let's now use this knowlege to perform\n",
    "\n",
    "$$ -\\sum x\\, \\log p(x) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFundie(predictionVector, expectedVector): # expected vector is an array of numbers eg [4, 3, 9, 8] which correspond to each of the output classes.\n",
    "    items = range(predictionVector.shape[0]) # 0 to 50000 iterator\n",
    "    return -predictionVector[items, expectedVector].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = lossFundie(predicitonsLogged, yTraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3218, grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enhanced LogMax!\n",
    "\n",
    "Then, there is a way to compute the log of the sum of exponentials in a more stable way, called the [LogSumExp trick](https://en.wikipedia.org/wiki/LogSumExp). The idea is to use the following formula:\n",
    "\n",
    "$$\\log \\left ( \\sum_{j=1}^{n} e^{x_{j}} \\right ) = \\log \\left ( e^{a} \\sum_{j=1}^{n} e^{x_{j}-a} \\right ) = a + \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}-a} \\right )$$\n",
    "\n",
    "where a is the maximum of the $x_{j}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logMaxEnhanced(predictions):\n",
    "    return predictions - predictions.logsumexp(-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsEnhanced = logMaxEnhanced(predicitonsLogged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "testNear(lossFundie(predictionsEnhanced, yTraining), loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all of the things that we need to tell our model that it is doing a bad job. Now let's move on to teaching it how to be better. The first step there is the \n",
    "\n",
    "Training loop\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "realLossFunction = Functional.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def accuracy(predictionVector, expectedVector):\n",
    "    return (torch.argmax(predictionVector, dim=1) == expectedVector).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputBatch = xTrainingNormalized[0:batchSize]\n",
    "expectedOutputBatch = yTraining[0:batchSize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = digitReconModel(inputBatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 10]),\n",
       " tensor([-0.3872, -0.5327, -0.4959,  0.3915, -0.0376,  0.3730,  0.1169, -0.1437,\n",
       "          0.2116,  0.4025], grad_fn=<SelectBackward>))"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape, predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsSoftMaxed = softMaxLoss(predictions).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 9, 8,  ..., 6, 9, 0])"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The numbers our model guessed\n",
    "torch.argmax(predictionsEnhanced, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.9480, -1.8775, -2.1438, -2.0242, -2.0767, -1.7691, -1.9394, -1.8847,\n",
       "        -1.9641, -2.0388, -2.0140, -2.0925, -2.0549, -2.0589, -2.0535, -1.9882,\n",
       "        -1.7559, -1.9391, -2.1218, -2.1305, -2.0132, -1.9379, -2.1349, -2.0537,\n",
       "        -1.5373, -1.7949, -2.0526, -1.9774, -1.7432, -2.0310, -2.0379, -1.7661,\n",
       "        -1.8911, -1.9774, -1.8800, -2.0218, -1.9286, -1.9246, -1.9899, -1.8869,\n",
       "        -2.1555, -1.8745, -2.0541, -2.0687, -2.0345, -1.8872, -2.0867, -1.9217,\n",
       "        -2.0325, -1.9956, -1.9493, -1.9070, -2.0198, -2.0715, -1.9224, -1.9747,\n",
       "        -1.9991, -2.1033, -1.9085, -2.0083, -2.1535, -1.9370, -2.0715, -1.9119],\n",
       "       grad_fn=<IndexBackward>)"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The largest prediction levels for each tuple.\n",
    "predictionsSoftMaxed[range(0, batchSize), torch.argmax(predictionsSoftMaxed, dim=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1250)"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(predictions, expectedOutputBatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About as good as random guessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "learningRate = 0.5\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get ready to train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range((rows//batchSize + 1)): # Process each of the batches\n",
    "        beginningRange = i * batchSize\n",
    "        endingRange = beginningRange + batchSize\n",
    "        _inputBatch = xTrainingNormalized[beginningRange:endingRange]\n",
    "        validationBatch = yTraining[beginningRange:endingRange]\n",
    "        preds = digitReconModel(_inputBatch)\n",
    "        loss = realLossFunction(preds, validationBatch)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for layer in digitReconModel.layers:\n",
    "                if hasattr(layer, 'weight'):\n",
    "                    layer.weight -= layer.weight.grad * learningRate\n",
    "                    layer.bias -= layer.bias.grad * learningRate\n",
    "                    layer.weight.grad.zero_()\n",
    "                    layer.bias.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1,\n",
       "        1, 3, 4, 3, 7, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7, 6, 1, 8, 7, 9, 3, 9, 8, 5,\n",
       "        5, 3, 3, 0, 7, 4, 9, 8, 0, 9, 4, 1, 4, 4, 6, 0])"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(digitReconModel(inputBatch), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1,\n",
       "        1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7, 6, 1, 8, 7, 9, 3, 9, 8, 5,\n",
       "        9, 3, 3, 0, 7, 4, 9, 8, 0, 9, 4, 1, 4, 4, 6, 0])"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expectedOutputBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9355)"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(digitReconModel(xTrainingNormalized[69:100]), yTraining[69:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay! Higher accuracy rates!\n",
    "\n",
    "**Moving on to refactoring the model to have that enhancement**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedDigitModel(torch.nn.Module): \n",
    "    def __init__(self, inputSize, numberHiddenLayers, classes):\n",
    "        super().__init__()\n",
    "        self.layerOne = torch.nn.Linear(inputSize, numberHiddenLayers)\n",
    "        self.layerTwo = torch.nn.Linear(numberHiddenLayers, classes)\n",
    "        \n",
    "    def __call__(self, inputMatrix): return self.layerTwo(Functional.relu(self.layerOne(inputMatrix)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhancedDigitModel = EnhancedDigitModel(columns, layerOutput, numberOfClasses.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = enhancedDigitModel(inputBatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 8, 0, 2, 8, 8, 2, 2, 8, 2, 2, 2, 2, 6, 8, 2, 6, 2, 3, 2, 2, 2, 2, 2,\n",
       "        2, 8, 8, 2, 2, 8, 2, 2, 6, 2, 8, 2, 2, 8, 8, 8, 8, 8, 8, 2, 2, 8, 2, 8,\n",
       "        9, 2, 6, 2, 8, 3, 2, 2, 8, 8, 2, 2, 8, 8, 3, 2])"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(predictions, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1,\n",
       "        1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7, 6, 1, 8, 7, 9, 3, 9, 8, 5,\n",
       "        9, 3, 3, 0, 7, 4, 9, 8, 0, 9, 4, 1, 4, 4, 6, 0])"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expectedOutputBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0781)"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(predictions, expectedOutputBatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "not very good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnhancedDigitModel(\n",
       "  (layerOne): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (layerTwo): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enhancedDigitModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range((rows//batchSize + 1)): # Process each of the batches\n",
    "        beginningRange = i * batchSize\n",
    "        endingRange = beginningRange + batchSize\n",
    "        _inputBatch = xTrainingNormalized[beginningRange:endingRange]\n",
    "        validationBatch = yTraining[beginningRange:endingRange]\n",
    "        preds = enhancedDigitModel(_inputBatch)\n",
    "        loss = realLossFunction(preds, validationBatch)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for layerParameter in enhancedDigitModel.parameters(): \n",
    "                layerParameter -= layerParameter.grad * learningRate\n",
    "            enhancedDigitModel.zero_grad() # method inherited from the nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9375)"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(enhancedDigitModel(inputBatch), expectedOutputBatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchData(iteration):\n",
    "    beginningRange = iteration * batchSize\n",
    "    endingRange = beginningRange + batchSize\n",
    "    _inputBatch = xTrainingNormalized[beginningRange:endingRange]\n",
    "    validationBatch = yTraining[beginningRange:endingRange]\n",
    "    return _inputBatch, validationBatch\n",
    "\n",
    "def trainModel(learningModel):\n",
    "    for i in range((rows)//batchSize + 1):\n",
    "        _inputBatch, _expectedBatch = batchData(i)\n",
    "        _predictions = learningModel(_inputBatch)\n",
    "        \n",
    "        loss = realLossFunction(_predictions, _expectedBatch)\n",
    "        \n",
    "        \n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for layerParameter in learningModel : layerParameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2611, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "realLossFunction(enhancedDigitModel(inputBatch), expectedOutputBatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'EnhancedDigitModel' object has no attribute 'backward'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-375-0654a03b7f58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menhancedDigitModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    537\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 539\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'EnhancedDigitModel' object has no attribute 'backward'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
